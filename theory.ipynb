{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Теоретическая основа \n",
    "\n",
    "- **ReLU** используется в слоях нейронной сети для создания нелинейных преобразований, которые позволяют сети моделировать более сложные функции.\n",
    "- **Adam** оптимизирует параметры сети, обновляя их в процессе обучения с учетом вычисленных градиентов, чтобы минимизировать функцию потерь.\n",
    "- **NLLLoss** измеряет, насколько хорошо модель предсказывает метки классов, и служит целевой функцией, которую Adam пытается минимизировать.\n",
    "\n",
    "\n",
    "### Что такое тензор?\n",
    "\n",
    "Тензор — это основная структура данных в PyTorch и других библиотеках для машинного обучения и глубокого обучения. Тензоры представляют собой многомерные массивы, которые можно использовать для хранения данных, таких как изображения, текст или числовые значения.\n",
    "\n",
    "В нашем случае `tensor([[0.0000, -43.4602]])` представляет собой выходной тензор нейронной сети, который имеет форму `(1, 2)`, где 1 — это размер батча (в данном случае один отзыв), а 2 — количество классов (положительный и отрицательный).\n",
    "\n",
    "- **Эти два числа представляют логарифмированные вероятности для каждого из классов.** Эти значения не являются вероятностями сами по себе, а являются результатом преобразования после применениея функции `log_softmax`.\n",
    "\n",
    "- **log_softmax:** На выходе последнего слоя нейронной сети (в данном случае `fc3`) могут быть сырые логиты, которые затем преобразуются в логарифмированные вероятности с помощью `log_softmax`. Это преобразование обеспечивает стабильность численных вычислений и упрощает вычисление функции потерь `NLLLoss`.\n",
    "\n",
    "- **Softmax:** Функция softmax принимает логиты и преобразует их в вероятности. Если применить softmax к логитам, то значения будут преобразованы в диапазон (0, 1) и их сумма будет равна 1. Однако, в данном случае используется `log_softmax`, что позволяет избежать вычислительных проблем, связанных с маленькими числами, и одновременно упрощает расчеты потерь.\n",
    "\n",
    "### Что такое логиты?\n",
    "\n",
    "Мое понимание: логит - это линейная комбинация признаков (уже с новыми полученными после обучения весами). Этот логит подается на вход функции активации (сигмоиде или, например, софтмаксу) и после этого мы получаем вероятности (в случае сигмоиды и софтмакса)\n",
    "\n",
    "1. **Логит функция**: Логит — это функция, обратная сигмоиде. Она преобразует вероятность p в логарифмические шансы. Формально: $\\text{logit}(p) = \\log\\left(\\frac{p}{1 - p}\\right)$.\n",
    "\n",
    "2. **Выходные значения нейронной сети**: На выходе из нейронной сети перед применением функции активации могут быть логиты. Логиты — это необработанные значения, которые могут быть преобразованы в вероятности с помощью функций активации, таких как Softmax или LogSoftmax.\n",
    "\n",
    "3. **Softmax**: Softmax принимает вектор чисел (логитов) и преобразует их в вероятности. Она нормирует значения так, чтобы сумма всех вероятностей была равна 1, и каждая вероятность находилась в интервале от 0 до 1.\n",
    "\n",
    "4. **LogSoftmax**: Это логарифм значений, полученных после применения Softmax. То есть, LogSoftmax — это $\\log(\\text{Softmax})$.\n",
    "\n",
    "Сигмоидная функция, также известная как логистическая функция, преобразует логиты (отношение шансов) в вероятности, ограничивая их в диапазоне от 0 до 1. Это позволяет интерпретировать результаты модели как вероятности принадлежности к положительному классу.\n",
    "\n",
    "Логиты могут быть линейной комбинацией признаков. В контексте логистической регрессии и нейронных сетей, логит часто представляет собой результат линейного преобразования входных признаков.\n",
    "\n",
    "1. **Линейная комбинация признаков**: В логистической регрессии (или любом другом линейном классификаторе) логит вычисляется как линейная комбинация входных признаков $ {x} $ с помощью весов $ {w} $ и смещения $ {b} $. Формально это записывается как:\n",
    "\n",
    "   $\n",
    "   z = w^T x + b\n",
    "   $\n",
    "\n",
    "   где $ { z } $ — это логит, $ { w } $ — вектор весов, $ { x } $ — вектор входных признаков, и $ { b } $ — смещение.\n",
    "\n",
    "2. **Применение функции активации**: После вычисления логита (линейной комбинации признаков) его часто пропускают через сигмоидную функцию (или другую функцию активации, такую как Softmax для многоклассовой классификации), чтобы получить вероятность:\n",
    "\n",
    "   $\n",
    "   p = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $\n",
    "\n",
    "   где $ { p } $ — это вероятность принадлежности к положительному классу.\n",
    "\n",
    "### Что такое функции активации?\n",
    "\n",
    "Функция активации — это математическая функция, применяемая в нейронной сети для введения нелинейности в модель. Она определяет выход нейрона, принимая на вход взвешенную сумму признаков (или выходные значения предыдущих слоев) и применяя к ней некоторую функцию. Это позволяет нейронным сетям моделировать сложные и нелинейные зависимости в данных.\n",
    "\n",
    "Вот несколько основных типов функций активации:\n",
    "\n",
    "1. **Сигмоида (Logistic Function)**:\n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "   - Преобразует входные значения в диапазон от 0 до 1.\n",
    "   - Используется для задач бинарной классификации.\n",
    "\n",
    "2. **Гиперболический тангенс (tanh)**:\n",
    "   $$\n",
    "   \\text{tanh}(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
    "   $$\n",
    "   - Преобразует входные значения в диапазон от -1 до 1.\n",
    "   - Часто используется в скрытых слоях нейронных сетей.\n",
    "\n",
    "3. **ReLU (Rectified Linear Unit)**:\n",
    "   $$\n",
    "   \\text{ReLU}(z) = \\max(0, z)\n",
    "   $$\n",
    "   - Преобразует входные значения так, что отрицательные значения становятся нулем, а положительные остаются неизменными.\n",
    "   - Широко используется в глубоких нейронных сетях, так как помогает бороться с проблемой исчезающего градиента.\n",
    "\n",
    "4. **Leaky ReLU**:\n",
    "   $$\n",
    "   \\text{Leaky ReLU}(z) = \\begin{cases} \n",
    "   z & \\text{если } z > 0 \\\\\n",
    "   \\alpha z & \\text{если } z \\leq 0\n",
    "   \\end{cases}\n",
    "   $$\n",
    "   - Модифицированная версия ReLU, которая позволяет небольшому отрицательному значению проходить через нейрон, чтобы избежать проблемы мертвых нейронов.\n",
    "\n",
    "5. **Softmax**:\n",
    "   $$\n",
    "   p_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n",
    "   $$\n",
    "   - Преобразует вектор логитов в вероятности для многоклассовой классификации.\n",
    "   - Используется на последнем слое для многоклассовых задач.\n",
    "\n",
    "6. **Swish**:\n",
    "   $$\n",
    "   \\text{Swish}(z) = z \\cdot \\sigma(z)\n",
    "   $$\n",
    "   - Плавная функция активации, которая может улучшить производительность в некоторых сетях.\n",
    "\n",
    "Функции активации помогают нейронным сетям учить сложные функции и делать прогнозы, которые не могли бы быть достигнуты с использованием только линейных комбинаций."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
