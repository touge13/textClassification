Программа классифицирует отзывы ресторана на положительные и отрицательные с помощью нейронной сети на PyTorch.

За теоретической основой предлагаю обратиться к файлу theory.ipynb: https://github.com/touge13/textClassification/blob/main/theory.ipynb

## Архитектура проекта:

1. **Импорт и обработка данных:**
   - Импортируем библиотеки: `nltk`, `pandas`, `sklearn`, `re`, `matplotlib` и `torch`.
   - Загружаем и предварительно обрабатываем данные: удаляем дубликаты, очищаем текст (удаляем знаки препинания и стоп-слова).
   - Преобразуем текст в числовой формат с помощью TF-IDF векторизации.

2. **Разделение данных:**
   - Разделяем данные на обучающую и тестовую выборки.

3. **Подготовка данных для PyTorch:**
   - Преобразуем данные в формат тензоров.

4. **Определение архитектуры нейронной сети:**
   - Создаем нейронную сеть с тремя полносвязными слоями:
     - `fc1`: Первый полносвязный слой
     - `fc2`: Второй полносвязный слой
     - `fc3`: Третий полносвязный слой для классификации
   - Применяем функцию активации ReLU после первых двух слоев.
   - На выходе используем логарифмированное значение softmax для получения вероятностей классов.

5. **Тренировка модели:**
   - Определяем оптимизатор (Adam) и функцию потерь (NLLLoss).
   - Тренируем модель на обучающих данных в течение 100 эпох.

6. **Предсказания:**
   - Применяем модель к новым отзывам для определения их полярности.

### Архитектура нейронной сети:

1. **Входные данные:**
   - Векторизованный при помощи TF-IDF текст.

2. **Слои нейронной сети:**
   - **Слой 1 (fc1):** Полносвязный слой, преобразующий входной вектор в вектор скрытого слоя.
   - **Активация ReLU:** Применяется к выходу первого полносвязного слоя.
   - **Слой 2 (fc2):** Полносвязный слой, преобразующий скрытый вектор в другой скрытый вектор.
   - **Активация ReLU:** Применяется к выходу второго полносвязного слоя.
   - **Слой 3 (fc3):** Полносвязный слой, преобразующий скрытый вектор в вектор выходных классов (положительный/отрицательный).
  
   Эти слои в нейронной сети выполняют разные задачи на пути к конечному предсказанию:

   1. **Слой 1 (fc1)**: Полносвязный (fully connected, fc) слой преобразует входной вектор признаков в вектор скрытого слоя. Этот слой захватывает информацию из входных данных и передает ее в скрытое пространство. Весы и смещения этого слоя обучаются для извлечения полезных признаков из исходных данных, что делает их более пригодными для решения задачи.
   
   2. **Активация ReLU после слоя 1**: Функция активации ReLU (Rectified Linear Unit) добавляет нелинейность в модель. Без активации сеть будет состоять только из линейных комбинаций, что сильно ограничит ее способность моделировать сложные зависимости. ReLU помогает решать эту проблему, обнуляя все отрицательные значения и позволяя сети обучать более сложные представления данных.
   
   3. **Слой 2 (fc2)**: Второй полносвязный слой добавляет еще один уровень абстракции. Он преобразует первый скрытый вектор в другой скрытый вектор. Каждый новый слой сети позволяет модели захватывать все более сложные зависимости в данных, и второй слой делает это на основании первого, работая как еще один уровень преобразований.
   
   4. **Активация ReLU после слоя 2**: После второго полносвязного слоя ReLU опять добавляет нелинейность. Это позволяет сети продолжать обучение сложных зависимостей в данных на более высоком уровне абстракции.
   
   5. **Слой 3 (fc3)**: Третий полносвязный слой преобразует последний скрытый вектор в вектор, представляющий выходные классы (например, положительный или отрицательный отзыв). Этот слой уже напрямую отвечает за предсказание, выдавая для каждого класса соответствующее значение (логит). Эти значения затем преобразуются функцией активации (например, софтмакс или сигмоида) в вероятности классов.
   
   Вся архитектура с несколькими слоями позволяет модели извлекать признаки с различных уровней сложности. Начальные слои могут "выучить" базовые шаблоны, тогда как более глубокие слои — более сложные, абстрактные признаки, что позволяет сети принимать лучшие решения для классификации данных.
   
3. **Выходные данные:**
   - Применение функции активации `log_softmax` для получения вероятностей классов.
