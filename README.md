Программа классифицирует отзывы ресторана на положительные и отрицательные с помощью нейронной сети на PyTorch.

## Архитектура проекта:

1. **Импорт и обработка данных:**
   - Импортируем библиотеки: `nltk`, `pandas`, `sklearn`, `re`, `matplotlib` и `torch`.
   - Загружаем и предварительно обрабатываем данные: удаляем дубликаты, очищаем текст (удаляем знаки препинания и стоп-слова).
   - Преобразуем текст в числовой формат с помощью TF-IDF векторизации.

2. **Разделение данных:**
   - Разделяем данные на обучающую и тестовую выборки.

3. **Подготовка данных для PyTorch:**
   - Преобразуем данные в формат тензоров.

4. **Определение архитектуры нейронной сети:**
   - Создаем нейронную сеть с тремя полносвязными слоями:
     - `fc1`: Первый полносвязный слой
     - `fc2`: Второй полносвязный слой
     - `fc3`: Третий полносвязный слой для классификации
   - Применяем функцию активации ReLU после первых двух слоев.
   - На выходе используем логарифмированное значение softmax для получения вероятностей классов.

5. **Тренировка модели:**
   - Определяем оптимизатор (Adam) и функцию потерь (NLLLoss).
   - Тренируем модель на обучающих данных в течение 100 эпох.

6. **Предсказания:**
   - Применяем модель к новым отзывам для определения их полярности.

### Архитектура нейронной сети:

1. **Входные данные:**
   - Векторизованный при помощи TF-IDF текст.

2. **Слои нейронной сети:**
   - **Слой 1 (fc1):** Полносвязный слой, преобразующий входной вектор в вектор скрытого слоя.
   - **Активация ReLU:** Применяется к выходу первого полносвязного слоя.
   - **Слой 2 (fc2):** Полносвязный слой, преобразующий скрытый вектор в другой скрытый вектор.
   - **Активация ReLU:** Применяется к выходу второго полносвязного слоя.
   - **Слой 3 (fc3):** Полносвязный слой, преобразующий скрытый вектор в вектор выходных классов (положительный/отрицательный).

3. **Выходные данные:**
   - Применение функции активации `log_softmax` для получения вероятностей классов.


## Теоретическая основа 

- **ReLU** используется в слоях нейронной сети для создания нелинейных преобразований, которые позволяют сети моделировать более сложные функции.
- **Adam** оптимизирует параметры сети, обновляя их в процессе обучения с учетом вычисленных градиентов, чтобы минимизировать функцию потерь.
- **NLLLoss** измеряет, насколько хорошо модель предсказывает метки классов, и служит целевой функцией, которую Adam пытается минимизировать.


### Что такое тензор?

Тензор — это основная структура данных в PyTorch и других библиотеках для машинного обучения и глубокого обучения. Тензоры представляют собой многомерные массивы, которые можно использовать для хранения данных, таких как изображения, текст или числовые значения.

В нашем случае `tensor([[0.0000, -43.4602]])` представляет собой выходной тензор нейронной сети, который имеет форму `(1, 2)`, где 1 — это размер батча (в данном случае один отзыв), а 2 — количество классов (положительный и отрицательный).

- **Эти два числа представляют логарифмированные вероятности для каждого из классов.** Эти значения не являются вероятностями сами по себе, а являются результатом преобразования после применениея функции `log_softmax`.

- **log_softmax:** На выходе последнего слоя нейронной сети (в данном случае `fc3`) могут быть сырые логиты, которые затем преобразуются в логарифмированные вероятности с помощью `log_softmax`. Это преобразование обеспечивает стабильность численных вычислений и упрощает вычисление функции потерь `NLLLoss`.

- **Softmax:** Функция softmax принимает логиты и преобразует их в вероятности. Если применить softmax к логитам, то значения будут преобразованы в диапазон (0, 1) и их сумма будет равна 1. Однако, в данном случае используется `log_softmax`, что позволяет избежать вычислительных проблем, связанных с маленькими числами, и одновременно упрощает расчеты потерь.

### Что такое логиты?

1. **Логит функция**: Логит — это функция, обратная сигмоиде. Она преобразует вероятность p в логарифмические шансы. Формально: $\text{logit}(p) = \log\left(\frac{p}{1 - p}\right)$.

2. **Выходные значения нейронной сети**: На выходе из нейронной сети перед применением функции активации могут быть логиты. Логиты — это необработанные значения, которые могут быть преобразованы в вероятности с помощью функций активации, таких как Softmax или LogSoftmax.

3. **Softmax**: Softmax принимает вектор чисел (логитов) и преобразует их в вероятности. Она нормирует значения так, чтобы сумма всех вероятностей была равна 1, и каждая вероятность находилась в интервале от 0 до 1.

4. **LogSoftmax**: Это логарифм значений, полученных после применения Softmax. То есть, LogSoftmax — это $\log(\text{Softmax})$.

Сигмоидная функция, также известная как логистическая функция, преобразует логиты (отношение шансов) в вероятности, ограничивая их в диапазоне от 0 до 1. Это позволяет интерпретировать результаты модели как вероятности принадлежности к положительному классу.

Логиты могут быть линейной комбинацией признаков. В контексте логистической регрессии и нейронных сетей, логит часто представляет собой результат линейного преобразования входных признаков.

1. **Линейная комбинация признаков**: В логистической регрессии (или любом другом линейном классификаторе) логит вычисляется как линейная комбинация входных признаков $ {x} $ с помощью весов $ {w} $ и смещения $ {b} $. Формально это записывается как:

   $
   z = w^T x + b
   $

   где $ { z } $ — это логит, $ { w } $ — вектор весов, $ { x } $ — вектор входных признаков, и $ { b } $ — смещение.

2. **Применение функции активации**: После вычисления логита (линейной комбинации признаков) его часто пропускают через сигмоидную функцию (или другую функцию активации, такую как Softmax для многоклассовой классификации), чтобы получить вероятность:

   $
   p = \sigma(z) = \frac{1}{1 + e^{-z}}
   $

   где $ { p } $ — это вероятность принадлежности к положительному классу.

### Что такое функции активации?

Функция активации — это математическая функция, применяемая в нейронной сети для введения нелинейности в модель. Она определяет выход нейрона, принимая на вход взвешенную сумму признаков (или выходные значения предыдущих слоев) и применяя к ней некоторую функцию. Это позволяет нейронным сетям моделировать сложные и нелинейные зависимости в данных.

Вот несколько основных типов функций активации:

1. **Сигмоида (Logistic Function)**:
   $$
   \sigma(z) = \frac{1}{1 + e^{-z}}
   $$
   - Преобразует входные значения в диапазон от 0 до 1.
   - Используется для задач бинарной классификации.

2. **Гиперболический тангенс (tanh)**:
   $$
   \text{tanh}(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
   $$
   - Преобразует входные значения в диапазон от -1 до 1.
   - Часто используется в скрытых слоях нейронных сетей.

3. **ReLU (Rectified Linear Unit)**:
   $$
   \text{ReLU}(z) = \max(0, z)
   $$
   - Преобразует входные значения так, что отрицательные значения становятся нулем, а положительные остаются неизменными.
   - Широко используется в глубоких нейронных сетях, так как помогает бороться с проблемой исчезающего градиента.

4. **Leaky ReLU**:
   $$
   \text{Leaky ReLU}(z) = \begin{cases} 
   z & \text{если } z > 0 \\
   \alpha z & \text{если } z \leq 0
   \end{cases}
   $$
   - Модифицированная версия ReLU, которая позволяет небольшому отрицательному значению проходить через нейрон, чтобы избежать проблемы мертвых нейронов.

5. **Softmax**:
   $$
   p_i = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}
   $$
   - Преобразует вектор логитов в вероятности для многоклассовой классификации.
   - Используется на последнем слое для многоклассовых задач.

6. **Swish**:
   $$
   \text{Swish}(z) = z \cdot \sigma(z)
   $$
   - Плавная функция активации, которая может улучшить производительность в некоторых сетях.

Функции активации помогают нейронным сетям учить сложные функции и делать прогнозы, которые не могли бы быть достигнуты с использованием только линейных комбинаций.